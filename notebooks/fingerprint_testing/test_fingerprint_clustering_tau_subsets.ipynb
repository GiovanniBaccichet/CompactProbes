{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 16\n",
    "selected_tau = 2.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.autonotebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = pd.read_csv(\"../../data/interim/string_df.csv\")\n",
    "\n",
    "balanced_pairs_df = pd.read_csv(\"../../data/train_test/test_pairs.csv\", index_col=0)\n",
    "\n",
    "combinations_df = pd.read_csv(\"../../data/train_test/10_combinations_df.csv\", index_col=0)\n",
    "\n",
    "balanced_pairs_df.drop_duplicates(inplace=True)\n",
    "balanced_pairs_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations_df = combinations_df[combinations_df['length'] > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df[\"concatenated\"] = string_df[\"concatenated\"].apply(\n",
    "    lambda x: np.array(list(x)).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Best Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def parse_log_file(filename):\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "        current_filter = None\n",
    "        current_threshold = None\n",
    "        current_min_error = None\n",
    "        current_confidence = None\n",
    "\n",
    "        for line in lines:\n",
    "            if \"Best Filter\" in line:\n",
    "                # Extract Best Filter using regex\n",
    "                filter_match = re.search(r\"Best Filter: (.+)\", line)\n",
    "                if filter_match:\n",
    "                    current_filter = filter_match.group(1).strip()\n",
    "\n",
    "            elif \"Best Threshold\" in line:\n",
    "                # Extract Best Threshold using regex\n",
    "                threshold_match = re.search(r\"Best Threshold: (.+)\", line)\n",
    "                if threshold_match:\n",
    "                    current_threshold = int(threshold_match.group(1).strip())\n",
    "\n",
    "            elif \"Min error\" in line:\n",
    "                # Extract Min Error using regex\n",
    "                min_error_match = re.search(r\"Min error: (.+)\", line)\n",
    "                if min_error_match:\n",
    "                    current_min_error = float(min_error_match.group(1).strip())\n",
    "\n",
    "            elif \"Confidence\" in line:\n",
    "                # Extract Confidence using regex\n",
    "                confidence_match = re.search(r\"Confidence: (.+)\", line)\n",
    "                if confidence_match:\n",
    "                    current_confidence = float(confidence_match.group(1).strip())\n",
    "\n",
    "                    # Once we have all values, create a tuple and add it to the data list\n",
    "                    data.append(\n",
    "                        (\n",
    "                            current_filter,\n",
    "                            current_threshold,\n",
    "                            current_min_error,\n",
    "                            current_confidence,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Reset current values for the next entry\n",
    "                    current_filter = None\n",
    "                    current_threshold = None\n",
    "                    current_min_error = None\n",
    "                    current_confidence = None\n",
    "\n",
    "    # Convert the list of tuples into a DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        data, columns=[\"Best Filter\", \"Best Threshold\", \"Min Error\", \"Confidence\"]\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "filename = \"../../reports/best_config\"\n",
    "best_configs_df = parse_log_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_configs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if M != 0:\n",
    "    best_configs_df = best_configs_df.head(M)\n",
    "\n",
    "if M == 0:\n",
    "    M = len(best_configs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_rate = len(string_df[\"concatenated\"].iloc[0]) / best_configs_df.shape[0]\n",
    "\n",
    "print(\"Compression Rate:\", compression_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_parser(input_string: str) -> list:\n",
    "    # Split the string into its parts\n",
    "    parts = input_string.split()\n",
    "\n",
    "    # Initialize the final array\n",
    "    result = []\n",
    "\n",
    "    # Process each part\n",
    "    for part in parts:\n",
    "        if part.startswith(\"0[\"):\n",
    "            # Extract the number inside the brackets\n",
    "            count = int(part[2:-1])\n",
    "            # Append the corresponding number of zeros to the result\n",
    "            result.extend([0] * count)\n",
    "        else:\n",
    "            # Translate the tiles to their respective values\n",
    "            for char in part:\n",
    "                if char == \"ðŸ€†\":\n",
    "                    result.append(-1)\n",
    "                elif char == \"ðŸ€«\":\n",
    "                    result.append(1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter(item, filter):\n",
    "    # item = np.array(list(item)).astype(int)\n",
    "    item = item.astype(int)\n",
    "    filter = filter_parser(filter)\n",
    "    return np.sum(np.multiply(item, filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_threshold(item, filter, threshold) -> int:\n",
    "    if apply_filter(item, filter) > threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filter_threshold_pair(item_1, item_2, filter, threshold) -> int:\n",
    "    if apply_filter_threshold(item_1, filter, threshold) == apply_filter_threshold(\n",
    "        item_2, filter, threshold\n",
    "    ):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(array1, array2, confidence):\n",
    "    # Check if arrays have the same length\n",
    "    if len(array1) != len(array2):\n",
    "        raise ValueError(\"Arrays must have the same length\")\n",
    "\n",
    "    # Initialize distance counter\n",
    "    distance = 0\n",
    "\n",
    "    # Iterate through arrays and count differences\n",
    "    for i in range(len(array1)):\n",
    "        if array1[i] != array2[i]:\n",
    "            distance += confidence[i]\n",
    "\n",
    "    distance = (distance / sum(confidence)) * len(confidence)\n",
    "\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fingerprint(item, best_filters, best_thresholds, confidence):\n",
    "    fingerprint = []\n",
    "\n",
    "    for best_filter, best_threshold in zip(best_filters, best_thresholds):\n",
    "        filtered = np.sum(np.multiply(item.astype(int), filter_parser(best_filter)))\n",
    "\n",
    "        if filtered > best_threshold:\n",
    "            filtered = 1\n",
    "        else:\n",
    "            filtered = -1\n",
    "\n",
    "        fingerprint.append(filtered)\n",
    "\n",
    "    return fingerprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprints = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in tqdm(string_df.iterrows(), total=string_df.shape[0]):\n",
    "    # Extracting best filters and thresholds from the main DataFrame\n",
    "    best_filters = best_configs_df[\"Best Filter\"].tolist()\n",
    "    best_thresholds = best_configs_df[\"Best Threshold\"].tolist()\n",
    "    confidence = best_configs_df[\"Confidence\"].tolist()\n",
    "\n",
    "    # Calculate the fingerprint using the relevant best filters and thresholds\n",
    "    fingerprint = calculate_fingerprint(\n",
    "        row[\"concatenated\"], best_filters, best_thresholds, confidence\n",
    "    )\n",
    "\n",
    "    # Store the result in the 'fprint' column\n",
    "    # string_df.at[i, \"fprint\"] = fingerprint\n",
    "    fingerprints.append(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df[\"fprint\"] = fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_remove = ['iPhone11_F', 'iPhone11_M', 'iPhoneXR_A', 'iPhoneXR_L', 'iPhone7_F', 'iPhone12_M', 'iPhone11_B', 'iPhone11_C', 'iPhone12Pro_C', 'S21Ultra_M', 'OppoFindX3Neo_A']\n",
    "string_df = string_df[~string_df['label'].isin(labels_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df = string_df.reset_index(inplace=False, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering w/ $\\tau$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df.iloc[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamming_distance(string_df.iloc[0, 2], string_df.iloc[1, 2], confidence) - selected_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(string_1, string_2, tau):\n",
    "    if hamming_distance(string_1, string_2, confidence) < tau:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(string_df.iloc[0, 2], string_df.iloc[100, 2], selected_tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(str1, str2):\n",
    "    if len(str1) != len(str2):\n",
    "        raise ValueError(\"Strings must be of the same length\")\n",
    "    return sum(c1 != c2 for c1, c2 in zip(str1, str2))\n",
    "\n",
    "def predict(string_1, string_2, tau):\n",
    "    return 1 if hamming_distance(string_1, string_2) < tau else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_dataset(string_df: pd.DataFrame, tau: float) -> np.ndarray:\n",
    "    n = len(string_df)\n",
    "    clusters = np.full(n, -1)  # -1 indicates that the item hasn't been clustered yet\n",
    "    next_cluster_id = 0\n",
    "    \n",
    "    for i in range(n):\n",
    "        if clusters[i] == -1:  # If not yet clustered\n",
    "            clusters[i] = next_cluster_id\n",
    "            for j in range(i + 1, n):\n",
    "                if predict(string_df['fprint'][i], string_df['fprint'][j], tau) == 1:\n",
    "                    clusters[j] = next_cluster_id\n",
    "            next_cluster_id += 1\n",
    "    \n",
    "    return clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters = cluster_dataset(string_df, selected_tau)\n",
    "\n",
    "# # Ground truth labels\n",
    "# true_labels = string_df['label'].to_numpy()\n",
    "\n",
    "# # Calculate metrics\n",
    "# homogeneity = homogeneity_score(true_labels, clusters)\n",
    "# completeness = completeness_score(true_labels, clusters)\n",
    "# v_measure = v_measure_score(true_labels, clusters)\n",
    "\n",
    "# print(f\"Homogeneity: {homogeneity}\")\n",
    "# print(f\"Completeness: {completeness}\")\n",
    "# print(f\"V-Measure: {v_measure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string_df['cluster'] = clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_list(s):\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in combinations_df.iterrows():\n",
    "    combination = index\n",
    "    length = row['length']\n",
    "\n",
    "    print('Combination:', combination)\n",
    "    combination_list = str_to_list(combination)\n",
    "    \n",
    "    subset_df = string_df[string_df['label'].isin(combination_list)]\n",
    "\n",
    "    print(subset_df)\n",
    "\n",
    "    clusters = cluster_dataset(subset_df, selected_tau)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
